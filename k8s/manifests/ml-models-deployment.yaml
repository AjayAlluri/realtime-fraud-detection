apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-models-service
  namespace: fraud-detection
  labels:
    app: ml-models-service
    component: ml-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-models-service
  template:
    metadata:
      labels:
        app: ml-models-service
    spec:
      containers:
      - name: ml-models
        image: fraud-detection/ml-models:latest
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: REDIS_HOST
          value: "redis-master-service"
        - name: REDIS_PORT
          value: "6379"
        - name: MODEL_CACHE_TTL
          value: "3600"
        - name: BATCH_SIZE
          value: "32"
        - name: MAX_WORKERS
          value: "4"
        - name: ENABLE_GPU
          value: "false"
        - name: PROMETHEUS_PORT
          value: "9090"
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2"
        volumeMounts:
        - name: model-storage
          mountPath: /app/models
        - name: model-config
          mountPath: /app/config
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 30
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: ml-models-pvc
      - name: model-config
        configMap:
          name: ml-models-config
      initContainers:
      - name: model-downloader
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Downloading ML models..."
          # Create model directories
          mkdir -p /models/xgboost /models/bert /models/sklearn
          
          # Placeholder for model download logic
          echo "Models would be downloaded here in production"
          
          # Create dummy model files for demonstration
          echo "dummy_xgboost_model" > /models/xgboost/fraud_classifier.json
          echo "dummy_bert_model" > /models/bert/config.json
          echo "dummy_isolation_forest" > /models/sklearn/isolation_forest.joblib
          
          echo "Model download completed"
        volumeMounts:
        - name: model-storage
          mountPath: /models
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tensorflow-serving
  namespace: fraud-detection
  labels:
    app: tensorflow-serving
    component: ml-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tensorflow-serving
  template:
    metadata:
      labels:
        app: tensorflow-serving
    spec:
      containers:
      - name: tensorflow-serving
        image: tensorflow/serving:2.13.0
        ports:
        - containerPort: 8501
          name: rest-api
        - containerPort: 8500
          name: grpc
        env:
        - name: MODEL_NAME
          value: "fraud_detection_ensemble"
        - name: MODEL_BASE_PATH
          value: "/models"
        - name: REST_API_PORT
          value: "8501"
        - name: GRPC_PORT
          value: "8500"
        - name: BATCHING_PARAMETERS_FILE
          value: "/config/batching_parameters.txt"
        resources:
          requests:
            memory: "1Gi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1"
        volumeMounts:
        - name: tensorflow-models
          mountPath: /models
        - name: tensorflow-config
          mountPath: /config
        livenessProbe:
          httpGet:
            path: /v1/models/fraud_detection_ensemble
            port: 8501
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /v1/models/fraud_detection_ensemble
            port: 8501
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: tensorflow-models
        persistentVolumeClaim:
          claimName: tensorflow-models-pvc
      - name: tensorflow-config
        configMap:
          name: tensorflow-serving-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-models-config
  namespace: fraud-detection
data:
  app_config.yaml: |
    # ML Models Service Configuration
    
    # Server settings
    server:
      host: "0.0.0.0"
      port: 8000
      workers: 4
      timeout: 30
      keepalive: 2
    
    # Model settings
    models:
      xgboost:
        path: "/app/models/xgboost/fraud_classifier.json"
        threshold: 0.5
        weight: 0.4
        cache_ttl: 3600
      
      bert:
        path: "/app/models/bert"
        model_name: "distilbert-base-uncased"
        max_length: 512
        weight: 0.3
        cache_ttl: 3600
      
      isolation_forest:
        path: "/app/models/sklearn/isolation_forest.joblib"
        contamination: 0.05
        weight: 0.2
        cache_ttl: 3600
      
      graph_neural_network:
        path: "/app/models/gnn"
        device: "cpu"
        weight: 0.1
        cache_ttl: 3600
    
    # Ensemble settings
    ensemble:
      method: "weighted_average"
      min_models: 2
      decision_threshold: 0.6
      confidence_threshold: 0.8
    
    # Performance settings
    performance:
      batch_size: 32
      max_batch_wait_ms: 100
      enable_caching: true
      cache_size: 10000
      enable_metrics: true
    
    # Logging
    logging:
      level: "INFO"
      format: "json"
      enable_performance_logging: true
    
    # Redis cache
    redis:
      host: "redis-master-service"
      port: 6379
      db: 0
      timeout: 5
      max_connections: 50
  
  prometheus.yml: |
    # Prometheus configuration for ML models service
    
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
    - job_name: 'ml-models-service'
      static_configs:
      - targets: ['localhost:9090']
      metrics_path: /metrics
      scrape_interval: 10s
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tensorflow-serving-config
  namespace: fraud-detection
data:
  batching_parameters.txt: |
    # TensorFlow Serving batching configuration
    
    # Maximum batch size
    max_batch_size { value: 128 }
    
    # Batch timeout in microseconds (100ms)
    batch_timeout_micros { value: 100000 }
    
    # Maximum number of enqueued batches
    max_enqueued_batches { value: 100 }
    
    # Number of batch threads
    num_batch_threads { value: 4 }
    
    # Allowed batch sizes
    allowed_batch_sizes { value: 1 }
    allowed_batch_sizes { value: 2 }
    allowed_batch_sizes { value: 4 }
    allowed_batch_sizes { value: 8 }
    allowed_batch_sizes { value: 16 }
    allowed_batch_sizes { value: 32 }
    allowed_batch_sizes { value: 64 }
    allowed_batch_sizes { value: 128 }
  
  models.config: |
    model_config_list {
      config {
        name: 'fraud_detection_ensemble'
        base_path: '/models/fraud_detection_ensemble/'
        model_platform: 'tensorflow'
        model_version_policy {
          latest {
            num_versions: 2
          }
        }
        version_labels {
          key: 'stable'
          value: 1
        }
        version_labels {
          key: 'canary'
          value: 2
        }
      }
    }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ml-models-pvc
  namespace: fraud-detection
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tensorflow-models-pvc
  namespace: fraud-detection
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi